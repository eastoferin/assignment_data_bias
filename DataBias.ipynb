{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRTBBETaSfbk",
        "outputId": "abc1abc6-d315-4d35-943c-04b968f32565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 10, 'score': {'value': 0.68408644, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.68408644, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n"
          ]
        }
      ],
      "source": [
        "from googleapiclient import discovery\n",
        "import json\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "API_KEY = 'AIzaSyC0Sqaou4IUy22wHQxuMjb1uc_fzgaAdnY'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "analyze_request = {\n",
        "  'comment': { 'text': 'i hate you' },\n",
        "  'requestedAttributes': {'TOXICITY': {}}\n",
        "}\n",
        "\n",
        "response = client.comments().analyze(body=analyze_request).execute()\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the API gives scores between 0 and 1. We can set the threshold to 0.5 and consider anything above 0.5 as toxic and below 0.5 as non toxic.\n"
      ],
      "metadata": {
        "id": "2xDaMRDytW_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Hypothesis:\n",
        "I believe that Perspective will be less able to detect toxicity from comments which incorporate British slang insults rather than common American ones. I think it will make more mistakes with comments in this category.\n",
        "\n",
        "Test set of statements\n",
        "\n",
        "American Insults:\n",
        "\n",
        "1. You're such an asshole.\n",
        "2. What a complete idiot.\n",
        "3. That makes you a thot.\n",
        "4. I don't know why you're a dickhead.\n",
        "5. This proves you're only meant for blue collar work.\n",
        "\n",
        "British Insults:\n",
        "\n",
        "1. You're such a tosser.\n",
        "2. What a complete wanker.\n",
        "3. That makes you a slag.\n",
        "4. I don't know why you're a knob head.\n",
        "5. This proves you're just a wazzock.\n",
        "\n",
        "Each British phrase corresponds to the American term used for the same listed number. They're effectively synonyms."
      ],
      "metadata": {
        "id": "6Of8GtMt01i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "comments_to_analyze = [\n",
        "    'You\\'re such an asshole.',\n",
        "    'You\\'re such a tosser.',\n",
        "    'What a complete idiot.',\n",
        "    'What a complete wanker.',\n",
        "    'That makes you a thot.',\n",
        "    'That makes you a slag.',\n",
        "    'I don\\'t know why you\\'re a dickhead.',\n",
        "    'I don\\'t know why you\\'re a knob head',\n",
        "    'This proves you\\'re just a blue collar',\n",
        "    'This proves you\\'re just a wazzock.',\n",
        "\n",
        "]\n",
        "\n",
        "for comment_text in comments_to_analyze:\n",
        "    # Define the analyze request for each comment\n",
        "    analyze_request = {\n",
        "        'comment': {'text': comment_text},\n",
        "        'requestedAttributes': {'TOXICITY': {}}\n",
        "    }\n",
        "\n",
        "    # Execute the analyze request\n",
        "    response = client.comments().analyze(body=analyze_request).execute()\n",
        "\n",
        "    # Print the response for each comment\n",
        "    print(f\"Comment: {comment_text}\")\n",
        "    print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvmNl5AoHEVE",
        "outputId": "a8ca6903-0f36-47e6-cb2f-e8af4f302ffa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comment: You're such an asshole.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 23, 'score': {'value': 0.95048624, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.95048624, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: You're such a tosser.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 21, 'score': {'value': 0.50789946, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.50789946, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: What a complete idiot.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 22, 'score': {'value': 0.8988238, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.8988238, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: What a complete wanker.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 23, 'score': {'value': 0.7510937, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.7510937, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: That makes you a thot.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 22, 'score': {'value': 0.50503236, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.50503236, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: That makes you a slag.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 22, 'score': {'value': 0.37751234, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.37751234, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: I don't know why you're a dickhead.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 35, 'score': {'value': 0.911035, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.911035, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: I don't know why you're a knob head\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 35, 'score': {'value': 0.52272606, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.52272606, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: This proves you're just a blue collar\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 37, 'score': {'value': 0.31963667, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.31963667, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n",
            "Comment: This proves you're just a wazzock.\n",
            "{'attributeScores': {'TOXICITY': {'spanScores': [{'begin': 0, 'end': 34, 'score': {'value': 0.4575469, 'type': 'PROBABILITY'}}], 'summaryScore': {'value': 0.4575469, 'type': 'PROBABILITY'}}}, 'languages': ['en'], 'detectedLanguages': ['en']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  \"You're such an asshole.\" This received a toxicity score of 0.95048624. Result: TOXIC\n",
        "\n",
        "2.  \"You're such a tosser.\" This received a toxicity score of 0.50789946. Result: TOXIC\n",
        "\n",
        "3. A \"What a complete idiot.\" This received a toxicity score of 0.8988238. Result: TOXIC\n",
        "\n",
        "4. B \"What a complete wanker.\" This received a toxicity score of 0.7510937. Result: TOXIC\n",
        "\n",
        "5. \"That makes you a thot.\" This received a toxicity score of 0.50503236. Result: TOXIC\n",
        "\n",
        "6. \"That makes you a slag.\" This received a toxicity score of 0.37751234. Result: NON TOXIC\n",
        "\n",
        "7. \"I don't know why you're a dickhead.\" This received a toxicity score of 0.911035. Result: TOXIC\n",
        "\n",
        "8. \"I don't know why you're a knob head.\" This received a toxicity score of 0.52272606. Result: TOXIC\n",
        "\n",
        "9. \"This proves you're just a blue collar.\" This received a toxicity score of 0.31963667. Result: NOT TOXIC\n",
        "\n",
        "10. \"This proves you're just a wazzock.\" This received a toxicity score of 0/4575469. Result: NOT TOXIC"
      ],
      "metadata": {
        "id": "OickIiqwIPoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, actual):\n",
        "    return accuracy_score(predictions, actual)\n",
        "\n",
        "def process_and_evaluate(scores, actual, country):\n",
        "    predictions = ['Yes' if score > 0.5 else 'No' for score in scores]\n",
        "    accuracy = calculate_accuracy(predictions, actual)\n",
        "    print(f\"Accuracy for {country} data = {accuracy}\")\n",
        "\n",
        "#yes indicates toxic and no indicates not toxic\n",
        "american_scores = [0.95948624, 0.8988238, 0.50503236, 0.911035, 0.31963667]\n",
        "american_actual = ['Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n",
        "\n",
        "british_scores = [0.50789946, 0.7510937, 0.37751234, 0.911035, 0.47575469]\n",
        "british_actual = ['Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n",
        "\n",
        "all_scores = british_scores + american_scores\n",
        "all_actual = british_actual + american_actual\n",
        "\n",
        "\n",
        "process_and_evaluate(all_scores, all_actual, \"all\")\n",
        "process_and_evaluate(british_scores, british_actual, \"British\")\n",
        "process_and_evaluate(american_scores, american_actual, \"American\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsWgHiDCGhOx",
        "outputId": "8621688b-2fb6-4a03-e8b5-8ccbcd6b26df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for all data = 0.7\n",
            "Accuracy for British data = 0.6\n",
            "Accuracy for American data = 0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for all data was a 0.7. However, the accuracy score for the british data was 0.6 and for American it was 0.8. This shows a slight but very real tendency towards not identifying British slang when toxic.:"
      ],
      "metadata": {
        "id": "SFpp281YF4au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My hypothesis was correct based on the samples I utilized. it was more likely to recognize the American terms as toxic and give it a higher rating despite the British terms being synonyms."
      ],
      "metadata": {
        "id": "4N3hdVWOqPKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write about Results:"
      ],
      "metadata": {
        "id": "ytLyCzPzvjRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this, I have learned that models cannot be assumed to account for diverse datasets. It's important to assess the model for any potential bias regarding slang words or terms from different age groups, subcultures, or regions of the world. I assumed that English language content would be the most tested for in this model, but it was clear from this test that American English is but not so much British English. I wasn't surprised from the overall results of these findings, but I didn't expect the difference to be as pronounced as it was.\n",
        "\n",
        "I theorize that this bias is based on the training data given to the model. I think it was trained on American information, so it struggled to recognize the British slang insults as toxic comments. I think this is a gap in the model\n",
        "\n",
        "Overall, the average is 0.717002 for the American sentences and 0.5233458 for the British sentences. This is a significant difference. Overall, two British returned as non-toxic and only one American sentence.\n",
        "\n",
        "We also saw this with the calculated accuracy at the end. It was less accurate for the British statements at 0.6 as compared to the American ones at 0.8.\n",
        "\n",
        "Interestingly, one of the lowest American sentences included the word \"thot\" which might be because it's a lesser known slang word more common amoung generation z online users rather than older audiences who worked on the model."
      ],
      "metadata": {
        "id": "T-WhbmdXvleq"
      }
    }
  ]
}